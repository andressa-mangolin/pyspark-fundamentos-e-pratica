{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efd0566-63a2-4e90-bad6-c3020f5d3a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **2. Tarefas e Aplicações Práticas com Dados**\n",
    "*(Notebook : `02_Manipulacao_e_IO_Basico`)*\n",
    "\n",
    "1 - Manipulação Básica (Tarefas que envolvem Listas e Dicionários)\n",
    "\n",
    "2 - Tratar Dados (Lógica de limpeza e validação de informações)\n",
    "\n",
    "3 - Funções (O ato de usar as funções criadas)\n",
    "\n",
    "4 - Ler/Escrever Arquivos (Operações de I/O para carregar e salvar dados)\n",
    "\n",
    "5 - Pandas (Uso da biblioteca para análise tabular)\n",
    "\n",
    "6 - ETL (Extrair, Transformar, Carregar) (Processo de engenharia de dados que usa todas as ferramentas)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bab732f-3212-4b0b-8a07-1dc5641a623b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1 - Manipulação Básica (Tarefas que envolvem Listas e Dicionários)\n",
    "\n",
    "**O que é**\n",
    "\n",
    "São as estruturas mais simples para guardar dados em Python.\n",
    "Quase tudo que você faz com dados começa com listas e dicionários.\n",
    "\n",
    "- Lista: coleção ordenada → [1,2,3]\n",
    "- Dicionário: dados em formato chave/valor → {\"nome\": \"Ana\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b441a70c-8010-4050-b32a-e0a280ee2569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ana\nMaria\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 1\n",
    "alunos = [\"Ana\", \"Pedro\", \"João\"]\n",
    "print(alunos[0])      # Ana\n",
    "\n",
    "cliente = {\"id\": 1, \"nome\": \"Maria\"}\n",
    "print(cliente[\"nome\"])  # Maria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472f341d-cb89-4549-a1d0-0ecfcae2d2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente João comprou pizza\nCliente João comprou refrigerante\nCliente João comprou doce\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 2 (prático (Engenharia de Dados))\n",
    "# Você recebe um JSON de API:\n",
    "registro = {\n",
    "    \"id\": 123,\n",
    "    \"nome\": \"João\",\n",
    "    \"compras\": [\"pizza\", \"refrigerante\", \"doce\"]\n",
    "}\n",
    "\n",
    "\n",
    "# A partir disso você:\n",
    "# - extrai data\n",
    "# - normaliza\n",
    "# - transforma em tabela\n",
    "\n",
    "for item in registro[\"compras\"]:\n",
    "    print(f\"Cliente {registro['nome']} comprou {item}\")\n",
    "\n",
    "\n",
    "# Usado para:\n",
    "# - entender payloads\n",
    "# - quebrar arrays\n",
    "# - organizar dados antes de enviar ao Spark, S3 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54af8a3-3870-48d7-8817-f03c382ff12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2 - Tratar Dados (Lógica de limpeza e validação de informações)\n",
    "\n",
    "**O que é**\n",
    "\n",
    "É garantir que os dados estejam corretos, completos e confiáveis antes de seguir para o restante do pipeline.\n",
    "\n",
    "Envolve:\n",
    "- remover valores inválidos\n",
    "- converter tipos\n",
    "- padronizar textos\n",
    "- remover duplicados\n",
    "- validar regras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335bb890-cf26-4352-8757-33dc5b7a25ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email inválido\n"
     ]
    }
   ],
   "source": [
    "#Exemplo 1\n",
    "idade = \"25\"\n",
    "idade = int(idade)  # Converte o texto \"25\" para o número 25, permitindo cálculos.\n",
    "\n",
    "nome = \"  ana  \"\n",
    "nome = nome.strip().title()  # Remove espaços extras e coloca a primeira letra em maiúscula (\"Ana\"). \n",
    "\n",
    "#.strip() remove os espaços das pontas, e .title() coloca a primeira letra em maiúscula.\n",
    "\n",
    "email = \"\"\n",
    "if not email:\n",
    "    print(\"Email inválido\")  # Verifica se a variável está vazia; se sim, avisa que é inválido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cabc7c1-a8ca-4ce4-a0e5-3d72e78064c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo prático (Engenharia de Dados)\n",
    "# Você recebe um CSV cheio de problemas:\n",
    "\n",
    "# id;nome;idade\n",
    "# 1;  JOAO ;20\n",
    "# 2;ANA; \n",
    "# 3;Maria;abc\n",
    "#arquivo: https://raw.githubusercontent.com/andressa-mangolin/pyspark-fundamentos-e-pratica/refs/heads/main/datasets/arquivo_teste.csv\n",
    "\n",
    "#Você trata no Spark:\n",
    "\n",
    "from pyspark.sql.functions import trim, col\n",
    "\n",
    "df = df.withColumn(\"nome\", trim(col(\"nome\")))\n",
    "df = df.filter(col(\"idade\").cast(\"int\").isNotNull())\n",
    "\n",
    "# Problemas resolvidos:\n",
    "# - espaços indesejados\n",
    "# - idade não numérica\n",
    "# - linhas vazias\n",
    "# - Isso é “tratamento”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f92ec897-a18a-4bfa-9ea7-a4e052aacd13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 46 bytes.\n✅ Arquivo de teste salvo com sucesso em: /Volumes/workspace/lhdw/landingzone/vendas/processar/arquivo_teste.csv\n\n--- DATAFRAME FINAL LIMPO (PROVA DE SUCESSO) ---\n+---+----+-----+\n| id|nome|idade|\n+---+----+-----+\n|  1|JOAO|   20|\n+---+----+-----+\n\n--- ESTRUTURA FINAL (Tipo de dado corrigido) ---\nroot\n |-- id: string (nullable = true)\n |-- nome: string (nullable = true)\n |-- idade: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# codigo completo\n",
    "import requests\n",
    "from pyspark.sql.functions import trim, col\n",
    "\n",
    "# Caminhos \n",
    "url = \"https://raw.githubusercontent.com/andressa-mangolin/pyspark-fundamentos-e-pratica/refs/heads/main/datasets/arquivo_teste.csv\"\n",
    "# Caminho de destino no Unity Catalog/DBFS:\n",
    "dbfs_path = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/arquivo_teste.csv\" \n",
    "\n",
    "# 1. Baixa o conteúdo do arquivo da URL (resolve o erro HTTPS)\n",
    "response = requests.get(url)\n",
    "response.raise_for_status() \n",
    "\n",
    "# 2. Salva o conteúdo no sistema de arquivos do Databricks (DBFS)\n",
    "dbutils.fs.put(\n",
    "    dbfs_path,\n",
    "    response.text,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Arquivo de teste salvo com sucesso em: {dbfs_path}\")\n",
    "\n",
    "# --- 2. LEITURA E TRATAMENTO (PYSPARK) ---\n",
    "\n",
    "# 1. Lê o DataFrame a partir do local onde foi salvo\n",
    "df = spark.read.csv(\n",
    "    dbfs_path,  \n",
    "    sep=\";\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "# 2. TRATAMENTO: Limpeza de espaços e remoção de linhas inválidas (Solução Definitiva)\n",
    "\n",
    "# A. Limpa espaços extras da coluna 'nome'\n",
    "df = df.withColumn(\"nome\", trim(col(\"nome\")))\n",
    "\n",
    "# B. Limpa espaços e aplica a conversão segura (try_cast) na 'idade'.\n",
    "# O try_cast resolve o erro 'CAST_INVALID_INPUT' retornando NULL se for 'abc' ou '\"\"'.\n",
    "df = df.withColumn(\"idade\", trim(col(\"idade\")))\n",
    "df = df.withColumn(\"idade\", col(\"idade\").try_cast(\"int\"))\n",
    "\n",
    "# C. Filtra: Remove todas as linhas onde o try_cast falhou (ou seja, onde a idade é NULL).\n",
    "df = df.filter(col(\"idade\").isNotNull())\n",
    "\n",
    "# --- 3. VALIDAÇÃO ---\n",
    "\n",
    "print(\"\\n--- DATAFRAME FINAL LIMPO (PROVA DE SUCESSO) ---\")\n",
    "df.show()\n",
    "\n",
    "print(\"--- ESTRUTURA FINAL (Tipo de dado corrigido) ---\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15eabe77-b7d9-43d3-8ac2-ce5bc099bcb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n| id|nome|idade|\n+---+----+-----+\n|  1|JOAO|   20|\n+---+----+-----+\n\nroot\n |-- id: string (nullable = true)\n |-- nome: string (nullable = true)\n |-- idade: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim, col\n",
    "\n",
    "# O DataFrame 'df' já está carregado do caminho dbfs_path.\n",
    "\n",
    "# 1. Limpa o NOME (remove espaços)\n",
    "df = df.withColumn(\"nome\", trim(col(\"nome\")))\n",
    "\n",
    "# 2. Limpa e Valida a IDADE (remove lixo e converte para número)\n",
    "# Limpa espaços e usa try_cast para converter string para INT de forma segura (sem travar).\n",
    "df = df.withColumn(\"idade\", trim(col(\"idade\")).try_cast(\"int\"))\n",
    "\n",
    "# 3. Filtra e remove todas as linhas inválidas (onde a idade virou NULL)\n",
    "df = df.filter(col(\"idade\").isNotNull())\n",
    "\n",
    "# Exibe o resultado limpo\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685b3dd0-6452-4f05-9865-c71224b80522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3 - Funções (O ato de usar as funções criadas)\n",
    "\n",
    "**O que é:**\n",
    "\n",
    "São blocos de código reaproveitáveis.\n",
    "Engenharia de dados usa MUITO para evitar repetir lógica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9474b711-d0eb-4fd0-b54f-d7dcbcc102aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#Exemplo simples\n",
    "# Define a função 'somar' que recebe dois valores (a e b).\n",
    "def somar(a, b):\n",
    "    # Devolve o resultado da soma de 'a' mais 'b'.\n",
    "    return a + b\n",
    "\n",
    "# Chama a função 'somar' com 5 e 3 e imprime o resultado (8).\n",
    "print(somar(5, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad1843d-54d5-49a2-96e5-ffc38ed4bd87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo prático \n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime # Necessário para o exemplo da data\n",
    "\n",
    "# --- PARTE 1: CPF ---limpar o CPF\n",
    "def limpar_cpf(cpf):\n",
    "    return cpf.replace(\".\", \"\").replace(\"-\", \"\")\n",
    "\n",
    "# PASSO CRÍTICO: Registra a função Python como UDF. \n",
    "# Isso cria a variável 'limpar_cpf_udf'.\n",
    "limpar_cpf_udf = udf(limpar_cpf, StringType())\n",
    "\n",
    "\n",
    "# E usa no pipeline:\n",
    "# Aplica a regra (limpar_cpf_udf) à coluna 'cpf'.\n",
    "df = df.withColumn(\"cpf_limpo\", limpar_cpf_udf(col(\"cpf\")))\n",
    "\n",
    "\n",
    "# --- PARTE 2: DATAS ---função para padronizar datas:\n",
    "# Define a regra Python para formatar a data\n",
    "def normalizar_data(data):\n",
    "    return datetime.strptime(data, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Para usar esta função em uma coluna, você também teria que registrá-la:\n",
    "# normalizar_data_udf = udf(normalizar_data, StringType())\n",
    "# df = df.withColumn(\"data_normalizada\", normalizar_data_udf(col(\"data_original\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8fe588-b804-4888-9868-45a90cb915ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345678900\n"
     ]
    }
   ],
   "source": [
    "# Ação: Define uma regra para limpar o CPF.\n",
    "def limpar_cpf(cpf):\n",
    "    # Regra: Remove pontos e hífens.\n",
    "    return cpf.replace(\".\", \"\").replace(\"-\", \"\")\n",
    "\n",
    "# --- TESTE ---\n",
    "cpf_sujo = \"123.456.789-00\"\n",
    "\n",
    "# Executa a regra com o valor de teste e imprime o resultado.\n",
    "print(limpar_cpf(cpf_sujo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d336ed62-2e39-41a2-bccb-e99c1729b84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 255 bytes.\n✅ Arquivo de teste salvo com sucesso em: /Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro.csv\n"
     ]
    }
   ],
   "source": [
    "# Caminhos \n",
    "url = \"https://raw.githubusercontent.com/andressa-mangolin/pyspark-fundamentos-e-pratica/refs/heads/main/datasets/clientes_cadastro.csv\"\n",
    "# Caminho de destino no Unity Catalog/DBFS:\n",
    "dbfs_path = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro.csv\" \n",
    "\n",
    "# 1. Baixa o conteúdo do arquivo da URL (resolve o erro HTTPS)\n",
    "response = requests.get(url)\n",
    "response.raise_for_status() \n",
    "\n",
    "# 2. Salva o conteúdo no sistema de arquivos do Databricks (DBFS)\n",
    "dbutils.fs.put(\n",
    "    dbfs_path,\n",
    "    response.text,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Arquivo de teste salvo com sucesso em: {dbfs_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f7f61cf-320d-453c-bbfc-ae006b722f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 255 bytes.\n✅ Arquivo de dados brutos salvo em: /Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro.csv\n\n\uD83D\uDCBE Arquivo CORRIGIDO salvo com sucesso em: /Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\n\n--- CONTEÚDO DO ARQUIVO SALVO (Válidos) ---\n+---+-----------+--------------+-----+-------+-----------+\n| id|       nome|  cpf_original|idade|  renda|  cpf_limpo|\n+---+-----------+--------------+-----+-------+-----------+\n|  1| João Silva|123.456.789-00|   30|2500.50|12345678900|\n|  2|Maria Souza|   99911122233|   45|4800.00|99911122233|\n|  3|Pedro Alves|   11122233344|   22|1500.00|11122233344|\n|  5| Rui Santos|   66677788899|   55|7000.00|66677788899|\n|  6| Paula Lima|   00000000000|   19| 900.00|00000000000|\n+---+-----------+--------------+-----+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, length\n",
    "import requests\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO E LEITURA (SETUP) ---\n",
    "\n",
    "# Define os caminhos do seu projeto\n",
    "url = \"https://raw.githubusercontent.com/andressa-mangolin/pyspark-fundamentos-e-pratica/refs/heads/main/datasets/clientes_cadastro.csv\"\n",
    "dbfs_path_leitura = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro.csv\" \n",
    "caminho_salvamento_final = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "\n",
    "# [Garante que o arquivo de leitura exista]\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "dbutils.fs.put(dbfs_path_leitura, response.text, overwrite=True)\n",
    "print(f\"✅ Arquivo de dados brutos salvo em: {dbfs_path_leitura}\")\n",
    "\n",
    "# Lê o DataFrame\n",
    "df = spark.read.csv(\n",
    "    dbfs_path_leitura,  \n",
    "    sep=\";\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "# --- 2. CORREÇÃO DE ERRO E LIMPEZA ---\n",
    "\n",
    "# CORRIGE O ERRO UNRESOLVED_COLUMN: Remove espaços em nomes de coluna.\n",
    "for nome_coluna in df.columns:\n",
    "    df = df.withColumnRenamed(nome_coluna, nome_coluna.strip())\n",
    "\n",
    "# Cria a coluna 'cpf_limpo' removendo TUDO que não é dígito (a 'Peneira Mágica').\n",
    "df = df.withColumn(\n",
    "    \"cpf_limpo\", \n",
    "    regexp_replace(col(\"cpf_original\"), \"[^0-9]\", \"\")\n",
    ")\n",
    "\n",
    "# --- 3. VALIDAÇÃO E FILTRAGEM ---\n",
    "\n",
    "# REGRA 1: O CPF ORIGINAL NÃO pode ser nulo.\n",
    "regra_nao_nulo = col(\"cpf_original\").isNotNull()\n",
    "\n",
    "# REGRA 2: O CPF LIMPO deve ter EXATAMENTE 11 dígitos.\n",
    "regra_tamanho_11 = (length(col(\"cpf_limpo\")) == 11)\n",
    "\n",
    "# REGRA FINAL: Combina as duas: deve ser NÃO NULO E ter 11 dígitos.\n",
    "REGRA_VALIDADE = regra_nao_nulo & regra_tamanho_11\n",
    "\n",
    "# Filtra o DataFrame, mantendo APENAS os registros que passaram em TODAS as regras.\n",
    "df_validos = df.where(REGRA_VALIDADE)\n",
    "\n",
    "# --- 4. SALVAMENTO E PROVA FINAL ---\n",
    "\n",
    "# Salva o DataFrame APENAS com os registros válidos no formato PARQUET.\n",
    "df_validos.write.mode(\"overwrite\").parquet(caminho_salvamento_final)\n",
    "print(f\"\\n\uD83D\uDCBE Arquivo CORRIGIDO salvo com sucesso em: {caminho_salvamento_final}\")\n",
    "\n",
    "# Mostra o resultado final (o que foi salvo).\n",
    "print(\"\\n--- CONTEÚDO DO ARQUIVO SALVO (Válidos) ---\")\n",
    "df_validos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7922260c-6a3d-4edc-acc1-090345a32938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Desmontagem e Reformatação do CPF\n",
    "\n",
    "A função **`substr(A, B)`** é utilizada para **cortar** o CPF limpo (sem pontos ou traços, ex: `12345678900`) em pedaços.\n",
    "\n",
    "O objetivo é alimentar o comando **`format_string`** para adicionar a pontuação de volta (`XXX.XXX.XXX-XX`).\n",
    "\n",
    "### Como a Função `substr(A, B)` Corta a String\n",
    "\n",
    "* **A** = Posição onde o corte **começa** (contando do 1).\n",
    "* **B** = **Quantidade** de caracteres a serem pegos a partir do início.\n",
    "\n",
    "| Peça | Código `substr(A, B)` | Início (A) | Quantidade (B) | O que a Peça Contém (Exemplo: 12345678900) |\n",
    "| :---: | :---: | :---: | :---: | :--- |\n",
    "| **1ª Peça** | `substr(1, 3)` | 1 | 3 | Pega os 3 primeiros dígitos. Resultado: **123** |\n",
    "| **2ª Peça** | `substr(4, 3)` | 4 | 3 | Começa no 4º dígito, pega 3. Resultado: **456** |\n",
    "| **3ª Peça** | `substr(7, 3)` | 7 | 3 | Começa no 7º dígito, pega 3. Resultado: **789** |\n",
    "| **4ª Peça** | `substr(10, 2)` | 10 | 2 | Pega os 2 últimos (dígitos verificadores). Resultado: **00** |\n",
    "\n",
    "---\n",
    "\n",
    "### Montagem Final\n",
    "\n",
    "O comando **`format_string`** usa esses quatro pedaços e os une, inserindo os separadores:\n",
    "\n",
    "`\"%s.%s.%s-%s\"` $\\rightarrow$ **`123.456.789-00`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9710a421-70a2-43d8-a115-ec1534b79a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 255 bytes.\n\nArquivo CORRIGIDO e FORMATADO salvo com sucesso em: /Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\n\n--- REGISTROS SALVOS (Com Formato Visual) ---\n+-----------+--------------+--------------+-----+\n|nome       |cpf_original  |cpf_formatado |idade|\n+-----------+--------------+--------------+-----+\n|João Silva |123.456.789-00|123.456.789-00|30   |\n|Maria Souza|99911122233   |999.111.222-33|45   |\n|Pedro Alves|11122233344   |111.222.333-44|22   |\n|Rui Santos |66677788899   |666.777.888-99|55   |\n|Paula Lima |00000000000   |000.000.000-00|19   |\n+-----------+--------------+--------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, length, format_string\n",
    "import requests\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO E LEITURA (SETUP) ---\n",
    "\n",
    "# Define os caminhos. Assumimos que o arquivo está na URL original.\n",
    "url = \"https://raw.githubusercontent.com/andressa-mangolin/pyspark-fundamentos-e-pratica/refs/heads/main/datasets/clientes_cadastro.csv\"\n",
    "dbfs_path_leitura = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro.csv\" \n",
    "caminho_salvamento_final = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "\n",
    "# [Garante que o arquivo de leitura exista]\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "dbutils.fs.put(dbfs_path_leitura, response.text, overwrite=True)\n",
    "\n",
    "# Lê o DataFrame\n",
    "df = spark.read.csv(\n",
    "    dbfs_path_leitura,  \n",
    "    sep=\";\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "# --- 2. PREPARAÇÃO E VALIDAÇÃO ---\n",
    "\n",
    "#  CORREÇÃO: Remove espaços em nomes de coluna (Resolve o erro UNRESOLVED_COLUMN).\n",
    "for nome_coluna in df.columns:\n",
    "    df = df.withColumnRenamed(nome_coluna, nome_coluna.strip())\n",
    "\n",
    "# A) LIMPEZA: Cria 'cpf_limpo' removendo tudo que não for dígito.\n",
    "df = df.withColumn(\n",
    "    \"cpf_limpo\", \n",
    "    regexp_replace(col(\"cpf_original\"), \"[^0-9]\", \"\")\n",
    ")\n",
    "\n",
    "# B) REGRA: O CPF tem que SER NÃO NULO E ter 11 dígitos.\n",
    "REGRA_VALIDADE = (col(\"cpf_original\").isNotNull()) & (length(col(\"cpf_limpo\")) == 11)\n",
    "\n",
    "# C) FILTRAGEM: Mantém APENAS os registros VÁLIDOS.\n",
    "df_validos = df.where(REGRA_VALIDADE)\n",
    "\n",
    "# --- 3. REFORMATAÇÃO VISUAL (PASSO NOVO) ---\n",
    "\n",
    "# Reverte o CPF limpo (11 dígitos) para o formato de exibição (XXX.XXX.XXX-XX).\n",
    "df_final = df_validos.withColumn(\n",
    "    \"cpf_formatado\",\n",
    "    format_string(\n",
    "        \"%s.%s.%s-%s\",\n",
    "        # Usa SUBSTRING para pegar as partes: 3 primeiros, 3 seguintes, 3 seguintes, 2 finais.\n",
    "        col(\"cpf_limpo\").substr(1, 3),\n",
    "        col(\"cpf_limpo\").substr(4, 3),\n",
    "        col(\"cpf_limpo\").substr(7, 3),\n",
    "        col(\"cpf_limpo\").substr(10, 2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 4. SALVAMENTO FINAL ---\n",
    "\n",
    "# Salva o DataFrame FINAL (validado e formatado) no formato PARQUET.\n",
    "df_final.write.mode(\"overwrite\").parquet(caminho_salvamento_final)\n",
    "print(f\"\\nArquivo CORRIGIDO e FORMATADO salvo com sucesso em: {caminho_salvamento_final}\")\n",
    "\n",
    "# --- 5. VERIFICAÇÃO ---\n",
    "print(\"\\n--- REGISTROS SALVOS (Com Formato Visual) ---\")\n",
    "# Mostra o resultado final, incluindo a coluna formatada.\n",
    "df_final.select(\"nome\", \"cpf_original\", \"cpf_formatado\", \"idade\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4516186-5be3-4187-a8bb-b17e79f235db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4 - Ler/Escrever Arquivos (Operações de I/O para carregar e salvar dados)\n",
    "**O que é**\n",
    "\n",
    "São operações para interagir com arquivos:\n",
    "\n",
    "- CSV\n",
    "- JSON\n",
    "- TXT\n",
    "- Parquet (no Spark)\n",
    "- S3, HDFS, Blob, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2f2ec6-416d-4187-a512-f543e891583a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# No PySpark, as operações de I/O são distribuídas e focam em formatos otimizados, como o Parquet, e sistemas de armazenamento em nuvem (como S3, que é muito usado no AWS).\n",
    "\n",
    "# Lendo Dados Brutos (Input):\n",
    "# Lê um arquivo CSV de um bucket S3 (fonte de dados brutos)\n",
    "# df = spark.read.csv(\"s3://meu-bucket/raw/clientes.csv\", header=True)\n",
    "\n",
    "#Salvando Dados Limpos (Output):\n",
    "# Escreve o resultado (df) no formato Parquet (otimizado e particionado)\n",
    "# Este é o coração de um pipeline, movendo dados para a zona 'bronze' ou 'trusted'.\n",
    "#df.write.parquet(\"s3://meu-bucket/bronze/clientes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa3e4d0d-46b5-4bae-8c5c-dfce986cbb9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Leitura e escrita completas.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# --- 1. LEITURA SIMPLES (INPUT) ---\n",
    "# Lê o arquivo CSV previamente salvo no DBFS\n",
    "df_input = spark.read.csv(\n",
    "    \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro.csv\",  \n",
    "    sep=\";\", \n",
    "    header=True\n",
    ")\n",
    "\n",
    "# Adiciona uma coluna de processamento mínima\n",
    "df_processado = df_input.withColumn(\"status\", lit(\"PROCESSADO\"))\n",
    "\n",
    "\n",
    "# --- 2. ESCRITA SIMPLES (OUTPUT) ---\n",
    "# Salva o resultado no formato Parquet\n",
    "df_processado.write.mode(\"overwrite\").parquet(\n",
    "    \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    ")\n",
    "\n",
    "print(\"✅ Leitura e escrita completas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e13a7d-c3e0-46b5-bcc7-4a4a9f3b07d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arquivo_teste.csv\nclientes_cadastro_corrigo.parquet\nclientes_cadastro.csv\ncustomers.csv\ngeolocation.csv\norder_items.csv\norder_payments.csv\norder_reviews.csv\norders.csv\nproduct_category_name_translation.csv\nproducts.csv\nsellers.csv\n"
     ]
    }
   ],
   "source": [
    "%sh ls /Volumes/workspace/lhdw/landingzone/vendas/processar/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f239aed-4e33-4de5-8a11-ba3ae1158200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 5 - Pandas (Uso da biblioteca para análise tabular)\n",
    "**O que é:**\n",
    "- Estrutura Principal: O Pandas organiza os dados em um objeto chamado DataFrame, que é uma tabela com linhas e colunas.\n",
    "- Finalidade: Ideal para tratamento, limpeza, análise e preparação de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed8054f-c0b5-4a50-81f2-63eb2c3552da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id         nome    cpf_original  idade   renda  idade_media\n0   1   João Silva  123.456.789-00     30  2500.5    33.166667\n1   2  Maria Souza     99911122233     45  4800.0    33.166667\n2   3  Pedro Alves     11122233344     22  1500.0    33.166667\n3   4    Ana Costa  444.555.666-AA     28  3200.0    33.166667\n4   5   Rui Santos     66677788899     55  7000.0    33.166667\n"
     ]
    }
   ],
   "source": [
    "# Exemplos de Uso Simples\n",
    "# Para começar a usar, você importa a biblioteca (o alias pd é o padrão) e carrega seus dados.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define o caminho real do arquivo CSV no Unity Catalog/DBFS\n",
    "caminho_csv = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro.csv\"\n",
    "\n",
    "# LÊ O ARQUIVO USANDO O CAMINHO COMPLETO E O SEPARADOR CORRETO\n",
    "df = pd.read_csv(caminho_csv, sep=';')\n",
    "\n",
    "# Calcula a média da coluna 'idade' e a armazena em uma nova coluna\n",
    "df[\"idade_media\"] = df[\"idade\"].mean()\n",
    "\n",
    "# Exibe as primeiras 5 linhas do DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0312c0b2-7f64-4810-bcdd-9eaf4aff12d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 114313\n-rwxrwxrwx 1 nobody nogroup       46 Dec 10 20:38 arquivo_teste.csv\ndrwxrwxrwx 2 nobody nogroup     4096 Dec 10 18:18 clientes_cadastro_corrigo.parquet\n-rwxrwxrwx 1 nobody nogroup      255 Dec 10 20:38 clientes_cadastro.csv\n-rwxrwxrwx 1 nobody nogroup       46 Dec  9 17:55 customers.csv\n-rwxrwxrwx 1 nobody nogroup 61273883 Dec  4 22:06 geolocation.csv\n-rwxrwxrwx 1 nobody nogroup 15438671 Dec  4 22:06 order_items.csv\n-rwxrwxrwx 1 nobody nogroup  5777138 Dec  4 22:06 order_payments.csv\n-rwxrwxrwx 1 nobody nogroup 14346950 Dec  4 22:06 order_reviews.csv\n-rwxrwxrwx 1 nobody nogroup 17654914 Dec  4 22:06 orders.csv\n-rwxrwxrwx 1 nobody nogroup     2542 Dec  4 22:06 product_category_name_translation.csv\n-rwxrwxrwx 1 nobody nogroup  2379446 Dec  4 22:06 products.csv\n-rwxrwxrwx 1 nobody nogroup   174703 Dec  4 22:06 sellers.csv\n"
     ]
    }
   ],
   "source": [
    "%sh ls -l /Volumes/workspace/lhdw/landingzone/vendas/processar/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a84866-e8ca-4b47-97dd-4f252be315f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####6 - ETL (Extrair, Transformar, Carregar) (Processo de engenharia de dados que usa todas as ferramentas)\n",
    "\n",
    "**O que é**\n",
    "\n",
    "É o processo principal de engenharia de dados e usa todos os tópicos acima.\n",
    "\n",
    "E = Extrair → pegar dados da origem\n",
    "\n",
    "T = Transformar → tratar / limpar / padronizar\n",
    "\n",
    "L = Carregar → salvar em outro formato ou banco\n",
    "\n",
    "As transformações mais usadas em ETL são sempre focadas em limpar, padronizar, enriquecer e estruturar os dados.\n",
    "Vou te passar a lista completa — as que realmente aparecem todos os dias no trabalho de Engenharia de Dados.\n",
    "\n",
    "## Resumo das Transformações mais Usadas no ETL\n",
    "\n",
    "| **Categoria**        | **Exemplos de Transformações**                        |\n",
    "|----------------------|--------------------------------------------------------|\n",
    "| **Limpeza**          | Remover nulos, eliminar duplicados, corrigir erros     |\n",
    "| **Casting**          | Converter `string → int`, `string → float`, `string → date` |\n",
    "| **Padronização**     | Padronizar nomes, cidades, normalizar texto            |\n",
    "| **Datas**            | Parse de datas, extrair ano/mês/dia, calcular idade    |\n",
    "| **Agregação**        | Soma, média, mínimo, máximo, contagem                  |\n",
    "| **Join**             | Unir tabelas com `inner`, `left`, `right`, `full`      |\n",
    "| **Enriquecimento**   | Adicionar informações externas (ex: tabela de referência) |\n",
    "| **Regras de negócio**| Criar flags, categorias, classificações                |\n",
    "| **Pivot**            | Transformar linhas em colunas ou colunas em linhas     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f8e84ac-5a84-46d4-b4ef-e1c2f87e572f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Limpeza de valores (Cleaning)\n",
    "O que é:\n",
    "\n",
    "Remover ou ajustar dados inválidos, nulos, duplicados ou inconsistentes.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- Remover linhas com valores vazios\n",
    "- Tratar “NULL”, “-”, “sem dado”\n",
    "- Remover espaços em excesso\n",
    "- Excluir duplicatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d92bed6-9d6b-4f71-9b50-ad8e7aa15b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+-----+-------+----------+\n| id|       nome|  cpf_original|idade|  renda|    status|\n+---+-----------+--------------+-----+-------+----------+\n|  2|Maria Souza|   99911122233|   45|4800.00|PROCESSADO|\n|  6| Paula Lima|   00000000000|   19| 900.00|PROCESSADO|\n|  4|  Ana Costa|444.555.666-AA|   28|3200.00|PROCESSADO|\n|  5| Rui Santos|   66677788899|   55|7000.00|PROCESSADO|\n|  1| João Silva|123.456.789-00|   30|2500.50|PROCESSADO|\n|  3|Pedro Alves|   11122233344|   22|1500.00|PROCESSADO|\n+---+-----------+--------------+-----+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim \n",
    "\n",
    "# Recarrega o DataFrame com o PySpark.\n",
    "# Isso garante que 'df' seja um objeto PySpark e reconheça o método '.withColumn'.\n",
    "caminho_parquet = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "df = spark.read.parquet(caminho_parquet) \n",
    "\n",
    "# 1. TRATAMENTO (Limpar espaços do 'nome')\n",
    "df = df.withColumn(\"nome\", trim(col(\"nome\")))\n",
    "\n",
    "# 2. FILTRAGEM (Remover nulos na 'idade')\n",
    "df = df.filter(col(\"idade\").isNotNull())\n",
    "\n",
    "# 3. DEDUPLICAÇÃO (Remover linhas duplicadas baseadas no 'id')\n",
    "df = df.dropDuplicates([\"id\"])\n",
    "\n",
    "# O DataFrame 'df' agora contém todas as transformações aplicadas.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0faad1-ffbb-4f1d-b5f0-c52b316e545a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ O código funcionou após recarregar com spark.read!\n+---+-----------+--------------+-----+-------+----------+\n| id|       nome|  cpf_original|idade|  renda|    status|\n+---+-----------+--------------+-----+-------+----------+\n|  2|Maria Souza|   99911122233|   45|4800.00|PROCESSADO|\n|  6| Paula Lima|   00000000000|   19| 900.00|PROCESSADO|\n|  4|  Ana Costa|444.555.666-AA|   28|3200.00|PROCESSADO|\n|  5| Rui Santos|   66677788899|   55|7000.00|PROCESSADO|\n|  1| João Silva|123.456.789-00|   30|2500.50|PROCESSADO|\n|  3|Pedro Alves|   11122233344|   22|1500.00|PROCESSADO|\n+---+-----------+--------------+-----+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim\n",
    "\n",
    "# Caminho do seu arquivo PARQUET corrigido\n",
    "caminho_parquet = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "\n",
    "# Carrega o arquivo usando o Spark. Isso garante que 'df' seja um objeto PySpark.\n",
    "df = spark.read.parquet(caminho_parquet)\n",
    "\n",
    "# Cria um novo DataFrame 'df_corrigido' com todas as etapas em sequência\n",
    "df_corrigido = df.withColumn(\"nome\", trim(col(\"nome\"))) \\\n",
    "                 .filter(col(\"idade\").isNotNull()) \\\n",
    "                 .dropDuplicates([\"id\"])\n",
    "\n",
    "print(\"✅ O código funcionou após recarregar com spark.read!\")\n",
    "df_corrigido.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7662245-b26e-4be4-91c8-643d9eaf1a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Cast de Tipos (Type Casting)\n",
    "\n",
    "**O que é:**\n",
    "\n",
    "Converter tipos para os corretos, ex: string → int, string → date.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- “25” → 25\n",
    "- “2023-10-01” → formato data real\n",
    "- “true” → boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5296f9-94ee-48c9-a784-6b0882753cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assume-se que 'df' é um PySpark DataFrame carregado.\n",
    "\n",
    "# 1. Converte o valor da coluna 'idade' (que era texto) para número inteiro (int).\n",
    "# Isso é obrigatório para fazer qualquer operação de média ou soma.\n",
    "#df = df.withColumn(\"idade\", col(\"idade\").cast(\"int\"))\n",
    "\n",
    "# 2. Converte o valor da coluna 'data' (que era texto) para o formato de calendário (date).\n",
    "# Isso é necessário para usar a coluna em filtros de tempo (ex: \"depois de Janeiro/2025\").\n",
    "#df = df.withColumn(\"data\", col(\"data\").cast(\"date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8f0fa4-d214-42fb-adc0-c84cba558f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim # Importa tudo que você precisará!\n",
    "from pyspark.sql import SparkSession # Garante que a sessão Spark esteja ativa (caso não esteja no Databricks)\n",
    "\n",
    "# Se você não estiver no Databricks, use isso para criar a sessão Spark:\n",
    "# spark = SparkSession.builder.appName(\"TipoConverter\").getOrCreate()\n",
    "\n",
    "# --- CAMINHO ---\n",
    "caminho_parquet = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "\n",
    "# --- CARREGAMENTO (Garante que 'df' é PySpark) ---\n",
    "try:\n",
    "    df = spark.read.parquet(caminho_parquet) \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo. Verifique se o caminho está 100% correto e se o arquivo existe: {e}\")\n",
    "    # Se o erro for aqui, o problema é no caminho ou na permissão!\n",
    "    # Não execute o resto do código se o carregamento falhar.\n",
    "    raise\n",
    "\n",
    "# 1. CONVERTE IDADE: Texto -> Número Inteiro (int)\n",
    "df = df.withColumn(\"idade\", col(\"idade\").cast(\"int\"))\n",
    "\n",
    "# 2. CONVERTE DATA: Texto -> Data de Calendário (date)\n",
    "# (Assumindo que 'data_cadastro' seja o nome correto da sua coluna de data)\n",
    "df = df.withColumn(\"data_cadastro\", col(\"data_cadastro\").cast(\"date\")) \n",
    "\n",
    "# 3. CONVERTE RENDA: Texto -> Número Decimal (double)\n",
    "# 'double' é usado para números com casas decimais (float)\n",
    "df = df.withColumn(\"renda\", col(\"renda\").cast(\"double\"))\n",
    "\n",
    "# 4. CONVERTE STATUS: Texto -> Número Curto (short)\n",
    "# 'short' é usado para números inteiros pequenos (códigos, IDs, etc.)\n",
    "df = df.withColumn(\"status\", col(\"status\").cast(\"short\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2480eb0-1920-4e4c-9692-b60382bfbe63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+-----+------+----------+\n| id|       nome|  cpf_original|idade| renda|    status|\n+---+-----------+--------------+-----+------+----------+\n|  2|Maria Souza|   99911122233|   45|4800.0|PROCESSADO|\n|  6| Paula Lima|   00000000000|   19| 900.0|PROCESSADO|\n|  4|  Ana Costa|444.555.666-AA|   28|3200.0|PROCESSADO|\n|  5| Rui Santos|   66677788899|   55|7000.0|PROCESSADO|\n|  1| João Silva|123.456.789-00|   30|2500.5|PROCESSADO|\n+---+-----------+--------------+-----+------+----------+\nonly showing top 5 rows\nroot\n |-- id: string (nullable = true)\n |-- nome: string (nullable = true)\n |-- cpf_original: string (nullable = true)\n |-- idade: integer (nullable = true)\n |-- renda: double (nullable = true)\n |-- status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# --- CAMINHO ---\n",
    "caminho_parquet = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "\n",
    "# --- CARREGAMENTO ---\n",
    "try:\n",
    "    df = spark.read.parquet(caminho_parquet) \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- TRANSFORMAÇÕES ENCADEDAS ---\n",
    "\n",
    "df_final = df.withColumn(\"nome\", trim(col(\"nome\"))) \\\n",
    "             .withColumn(\"idade\", col(\"idade\").cast(\"int\")) \\\n",
    "             .withColumn(\"renda\", col(\"renda\").cast(\"double\")) \\\n",
    "             .filter(col(\"idade\").isNotNull()) \\\n",
    "             .dropDuplicates([\"id\"])\n",
    "\n",
    "\n",
    "# Exibe os dados transformados\n",
    "df_final.show(5)\n",
    "\n",
    "# Mostra a nova estrutura para confirmar os tipos (int e double)\n",
    "df_final.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4a372b-729f-49c0-9707-f67bb9e1d042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Padronização de Texto\n",
    "**O que é:**\n",
    "\n",
    "Corrigir, formatar e deixar os textos uniformes.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- UPPER / lower\n",
    "- Remover acentos\n",
    "- Corrigir variações (“Joao” → “João”)\n",
    "- Normalizar nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7fcd83-e34d-4a9c-ab9f-f17a1e3224db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "df = df.withColumn(\"cidade\", upper(col(\"cidade\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b839ae2-00e5-4055-a4f0-c21a836554b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+-----+-------+----------+\n| id|       nome|  cpf_original|idade|  renda|    status|\n+---+-----------+--------------+-----+-------+----------+\n|  1| JOÃO SILVA|123.456.789-00|   30|2500.50|PROCESSADO|\n|  2|MARIA SOUZA|   99911122233|   45|4800.00|PROCESSADO|\n|  3|PEDRO ALVES|   11122233344|   22|1500.00|PROCESSADO|\n|  4|  ANA COSTA|444.555.666-AA|   28|3200.00|PROCESSADO|\n|  5| RUI SANTOS|   66677788899|   55|7000.00|PROCESSADO|\n+---+-----------+--------------+-----+-------+----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Exemplo MAIÚSCULAS\n",
    "from pyspark.sql.functions import col, upper\n",
    "\n",
    "# --- CARREGAMENTO (Assumindo que 'df' é carregado) ---\n",
    "caminho_parquet = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "df = spark.read.parquet(caminho_parquet) \n",
    "\n",
    "# 1. Padroniza a coluna 'nome' para MAIÚSCULAS para facilitar buscas e agrupamentos.\n",
    "df = df.withColumn(\"nome\", upper(col(\"nome\")))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e72a29d-11ba-452f-a00a-b40f3ca2be9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+-----+-------+----------+\n| id|       nome|  cpf_original|idade|  renda|    status|\n+---+-----------+--------------+-----+-------+----------+\n|  1| joão silva|123.456.789-00|   30|2500.50|PROCESSADO|\n|  2|maria souza|   99911122233|   45|4800.00|PROCESSADO|\n|  3|pedro alves|   11122233344|   22|1500.00|PROCESSADO|\n|  4|  ana costa|444.555.666-AA|   28|3200.00|PROCESSADO|\n|  5| rui santos|   66677788899|   55|7000.00|PROCESSADO|\n+---+-----------+--------------+-----+-------+----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Exemplo minusculo\n",
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "caminho_parquet = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "df = spark.read.parquet(caminho_parquet) \n",
    "\n",
    "# 1. Padroniza a coluna 'nome' para MAIÚSCULAS para facilitar buscas e agrupamentos.\n",
    "df = df.withColumn(\"nome\", lower(col(\"nome\")))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eaabe22-2798-46d9-b00b-78717077be2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+-----+-------+----------+-----------+\n| id|       nome|  cpf_original|idade|  renda|    status| nome_limpo|\n+---+-----------+--------------+-----+-------+----------+-----------+\n|  2|Maria Souza|   99911122233|   45|4800.00|PROCESSADO|MARIA SOUZA|\n|  6| Paula Lima|   00000000000|   19| 900.00|PROCESSADO| PAULA LIMA|\n|  4|  Ana Costa|444.555.666-AA|   28|3200.00|PROCESSADO|  ANA COSTA|\n|  5| Rui Santos|   66677788899|   55|7000.00|PROCESSADO| RUI SANTOS|\n|  1| João Silva|123.456.789-00|   30|2500.50|PROCESSADO| JOAO SILVA|\n+---+-----------+--------------+-----+-------+----------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, upper, trim\n",
    "\n",
    "caminho_parquet = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "df = spark.read.parquet(caminho_parquet) \n",
    "\n",
    "df_limpo = df.withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome\"), \"[ÁÀÂÃ]\", \"A\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[ÉÈÊ]\", \"E\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[ÍÌÎ]\", \"I\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[ÓÒÔÕ]\", \"O\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[ÚÙÛÜ]\", \"U\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[Ç]\", \"C\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[áàâã]\", \"a\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[éèê]\", \"e\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[íìî]\", \"i\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[óòôõ]\", \"o\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[úùûü]\", \"u\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        regexp_replace(col(\"nome_limpo\"), \"[ç]\", \"c\")) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        trim(col(\"nome_limpo\"))) \\\n",
    "    .withColumn(\"nome_limpo\", \n",
    "        upper(col(\"nome_limpo\"))) \\\n",
    "    .dropDuplicates([\"id\"]) # Suas outras transformações\n",
    "\n",
    "# O resultado: 'João Silva' vira 'JOAO SILVA'\n",
    "df_limpo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea608125-1d25-4552-b549-1831ca4fc887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+-----+-------+----------+-----------+\n| id|       nome|  cpf_original|idade|  renda|    status| nome_limpo|\n+---+-----------+--------------+-----+-------+----------+-----------+\n|  1| João Silva|123.456.789-00|   30|2500.50|PROCESSADO| JOAO SILVA|\n|  2|Maria Souza|   99911122233|   45|4800.00|PROCESSADO|MARIA SOUZA|\n|  3|Pedro Alves|   11122233344|   22|1500.00|PROCESSADO|PEDRO ALVES|\n|  4|  Ana Costa|444.555.666-AA|   28|3200.00|PROCESSADO|  ANA COSTA|\n|  5| Rui Santos|   66677788899|   55|7000.00|PROCESSADO| RUI SANTOS|\n+---+-----------+--------------+-----+-------+----------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# CARACTERES_ORIGEM = \"áàâãéèêíìîóòôõúùûüçÁÀÂÃÉÈÊÍÌÎÓÒÔÕÚÙÛÜÇ\"\n",
    "# CARACTERES_DESTINO = \"aaaaeeeiiioooouuuucAAAAEEEIIIOOOOUUUUC\"\n",
    "\n",
    "# 1ª Linha: TIRA OS ACENTOS\n",
    "# Cria a coluna 'nome_limpo'. Pega a coluna 'nome' original e troca todos os caracteres \n",
    "# acentuados (ã, é, ç) pelos seus equivalentes sem acento (a, e, c).\n",
    "\n",
    "# 2ª Linha: COLOCA EM MAIÚSCULAS\n",
    "# Pega o resultado sem acentos da linha anterior (nome_limpo) e converte TODAS as letras para MAIÚSCULAS.           \n",
    "\n",
    "# 3ª Linha: REMOVE ESPAÇOS\n",
    "# Pega o resultado em maiúsculas da linha anterior e remove qualquer espaço em branco \n",
    "# que tenha sobrado no início ou no fim do texto (limpeza final).\n",
    "            \n",
    "\n",
    "from pyspark.sql.functions import col, translate, upper, trim\n",
    "\n",
    "# Defina as strings de origem e destino\n",
    "CARACTERES_ORIGEM = \"áàâãéèêíìîóòôõúùûüçÁÀÂÃÉÈÊÍÌÎÓÒÔÕÚÙÛÜÇ\"\n",
    "CARACTERES_DESTINO = \"aaaaeeeiiioooouuuucAAAAEEEIIIOOOOUUUUC\"\n",
    "\n",
    "# --- TRANSFORMAÇÃO ---\n",
    "df_final = df.withColumn(\"nome_limpo\", translate(col(\"nome\"), CARACTERES_ORIGEM, CARACTERES_DESTINO)) \\\n",
    "             .withColumn(\"nome_limpo\", upper(col(\"nome_limpo\"))) \\\n",
    "             .withColumn(\"nome_limpo\", trim(col(\"nome_limpo\")))\n",
    "\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24fb4477-0616-4e61-92c2-bfda8bfd9ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Normalização de Dados (Standardization)\n",
    "\n",
    "**O que é:**\n",
    "\n",
    "Padronizar unidades ou formatos.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- 10 km → 10.000 \n",
    "- R$ 1.200,00 → 1200.00\n",
    "- \"Sim/Não\" → 1/0\n",
    "- \"F/M\" → \"Feminino/Masculino\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29397cbb-cb59-43fa-aa83-1f5c240c0b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"valor\", col(\"valor\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde3bbbc-045b-4005-adc5-8c6cff86413b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df.withColumn(\"genero_completo\", # Cria uma nova coluna para o resultado\n",
    "             when(col(\"genero\") == \"F\", \"Feminino\") # SE 'genero' for \"F\", então use \"Feminino\"\n",
    "             .when(col(\"genero\") == \"M\", \"Masculino\") # SE 'genero' for \"M\", então use \"Masculino\"\n",
    "             .otherwise(col(\"genero\")) # Caso contrário (se for Nulo ou outro código), mantém o valor original.\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2aea420-07fd-4f63-8e01-349ced4ed012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- nome: string (nullable = true)\n |-- cpf_original: string (nullable = true)\n |-- idade: integer (nullable = true)\n |-- renda: double (nullable = true)\n |-- status: string (nullable = true)\n |-- status_processado_flag: integer (nullable = false)\n\n+---+-----------+--------------+-----+------+----------+----------------------+\n| id|       nome|  cpf_original|idade| renda|    status|status_processado_flag|\n+---+-----------+--------------+-----+------+----------+----------------------+\n|  2|MARIA SOUZA|   99911122233|   45|4800.0|PROCESSADO|                     0|\n|  6| PAULA LIMA|   00000000000|   19| 900.0|PROCESSADO|                     0|\n|  4|  ANA COSTA|444.555.666-AA|   28|3200.0|PROCESSADO|                     0|\n|  5| RUI SANTOS|   66677788899|   55|7000.0|PROCESSADO|                     0|\n|  1| JOAO SILVA|123.456.789-00|   30|2500.5|PROCESSADO|                     0|\n+---+-----------+--------------+-----+------+----------+----------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, upper, translate, when, lit\n",
    "\n",
    "# Strings para a função translate (remoção de acentos)\n",
    "CARACTERES_ORIGEM = \"áàâãéèêíìîóòôõúùûüçÁÀÂÃÉÈÊÍÌÎÓÒÔÕÚÙÛÜÇ\"\n",
    "CARACTERES_DESTINO = \"aaaaeeeiiioooouuuucAAAAEEEIIIOOOOUUUUC\"\n",
    "\n",
    "# Caminho do seu arquivo Parquet\n",
    "caminho_parquet = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "df = spark.read.parquet(caminho_parquet)\n",
    "\n",
    "df_final = df.withColumn(\"nome\", \n",
    "             # Padroniza 'nome': Remove acentos (translate), coloca em MAIÚSCULAS (upper) e remove espaços (trim)\n",
    "             trim(upper(translate(col(\"nome\"), CARACTERES_ORIGEM, CARACTERES_DESTINO)))\n",
    "             ) \\\n",
    "             .withColumn(\"idade\", \n",
    "                 # Normalização de Tipo: Converte 'idade' para Número Inteiro (int)\n",
    "                 col(\"idade\").cast(\"int\")) \\\n",
    "             .withColumn(\"renda\", \n",
    "                 # Normalização de Tipo: Converte 'renda' para Número Decimal (double)\n",
    "                 col(\"renda\").cast(\"double\")) \\\n",
    "             .withColumn(\"status_processado_flag\", \n",
    "                 # Normalização de Status: Converte Status para FLAG numérica (1 para Processado, 0 caso contrário)\n",
    "                 when(col(\"status\") == \"Processado\", 1).otherwise(0)) \\\n",
    "             .filter(\n",
    "                 # Filtragem: Remove linhas onde a idade é nula, garantindo a qualidade mínima dos dados\n",
    "                 col(\"idade\").isNotNull()) \\\n",
    "             .dropDuplicates(\n",
    "                 # Deduplicação: Remove duplicatas do registro, usando o 'id' como chave\n",
    "                 [\"id\"])\n",
    "\n",
    "df_final.printSchema() # Confirma se as conversões (int, double, flag) funcionaram\n",
    "\n",
    "df_final.show(5) # Mostra as 5 primeiras linhas do resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d363309b-950d-4072-8341-c735007e3f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Tratamento de Datas\n",
    "**O que é:**\n",
    "\n",
    "Corrigir formatos e extrair partes da data.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- Converter “01/10/2025” → “2025-10-01”\n",
    "- Extrair mês, ano, dia\n",
    "- Criar coluna de período (YYYY-MM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527489c2-5f5a-45f4-96cc-c475f582a74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Converte a coluna 'data' (que era texto) em um formato de calendário real (DATE).\n",
    "df = df.withColumn(\"data\", to_date(col(\"data\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# 2. Cria a coluna 'ano', extraindo o número do ano da coluna 'data'.\n",
    "df = df.withColumn(\"ano\", year(col(\"data\")))\n",
    "\n",
    "# 3. Cria a coluna 'mes', extraindo o número do mês da coluna 'data'.\n",
    "df = df.withColumn(\"mes\", month(col(\"data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70d4ca2-3235-413d-9186-8b2cd5842059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n|data_original|data_convertida|\n+-------------+---------------+\n|15/12/2025   |2025-12-15     |\n|01/01/2024   |2024-01-01     |\n|202501       |NULL           |\n+-------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, try_to_date # Usamos try_to_date para ser seguro\n",
    "\n",
    "dados_teste = [(\"15/12/2025\",), (\"01/01/2024\",), (\"202501\",)] \n",
    "df_teste = spark.createDataFrame(dados_teste, [\"data_original\"])\n",
    "\n",
    "df_resultado = df_teste.withColumn(\"data_convertida\", \n",
    "             # TENTA converter de \"dd/MM/yyyy\" para tipo DATE.\n",
    "             try_to_date(col(\"data_original\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "df_resultado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e308d6bb-aae8-47b4-8f7f-c688577759d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+---+\n|      data| ano|mes|dia|\n+----------+----+---+---+\n|2025-12-15|2025| 12| 15|\n|2024-01-01|2024|  1|  1|\n+----------+----+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth\n",
    "\n",
    "# 2. Cria uma tabela (DataFrame) com datas\n",
    "dados_teste = [(\"2025-12-15\",), (\"2024-01-01\",)]\n",
    "colunas = [\"data\"]\n",
    "df_teste = spark.createDataFrame(dados_teste, colunas).withColumn(\"data\", col(\"data\").cast(\"date\")) \n",
    "\n",
    "# 3. APLICA SUA LINHA: Extrai o ano e coloca na coluna 'ano'\n",
    "df_resultado = df_teste.withColumn(\"ano\", year(col(\"data\"))) \\\n",
    "                       .withColumn(\"mes\", month(col(\"data\"))) \\\n",
    "                       .withColumn(\"dia\", dayofmonth(col(\"data\")))\n",
    "\n",
    "# 4. Mostra o resultado final\n",
    "df_resultado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba177c7-bb18-444c-9cd5-4e9fb67af55e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Agregações (Aggregation)\n",
    "**O que é:**\n",
    "\n",
    "Agrupar e sumarizar dados.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- Contar quantos clientes\n",
    "- Somar vendas por mês\n",
    "- Calcular idade média por cidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb037405-85be-4d14-ac4e-b2cfc31e607e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"cidade\").agg({\"idade\": \"avg\", \"id\": \"count\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf2fd8a-bf4b-4db6-894d-69e557a87af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resultado da Média e Contagem Global de Clientes ---\n+------------------+------------------+--------------+\n|media_renda_global|media_idade_global|total_clientes|\n+------------------+------------------+--------------+\n|3316.75           |33.166666666666664|6             |\n+------------------+------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, count, col\n",
    "\n",
    "caminho_parquet = \"/Volumes/workspace/lhdw/landingzone/vendas/processar/clientes_cadastro_corrigo.parquet\"\n",
    "df_clientes = spark.read.parquet(caminho_parquet) # DataFrame carregado em df_clientes\n",
    "\n",
    "# Calcula a Média de Renda, Média de Idade e o Total de Clientes\n",
    "df_resultado = df_clientes.agg(\n",
    "    avg(col(\"renda\")).alias(\"media_renda_global\"),\n",
    "    avg(col(\"idade\")).alias(\"media_idade_global\"),\n",
    "    count(col(\"id\")).alias(\"total_clientes\") # Usando 'id' para a contagem\n",
    ")\n",
    "\n",
    "print(\"--- Resultado da Média e Contagem Global de Clientes ---\")\n",
    "df_resultado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8467e71f-a0fc-4049-86e0-371f0c309d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Junções (Joins)\n",
    "**O que é:**\n",
    "\n",
    "Combinar tabelas, algo ESSENCIAL em engenharia de dados.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- Unir clientes + compras\n",
    "- Tabela de produtos + preços\n",
    "- Fato + dimensões\n",
    "\n",
    "\n",
    "###Os 4 Tipos Principais de Junção (`Join`) em PySpark\n",
    "\n",
    "Os Joins são cruciais para **combinar informações de duas tabelas** (DataFrames) com base em um campo em comum (a chave de ligação).\n",
    "\n",
    "#### 1. `INNER JOIN` (Junção Interna)\n",
    "\n",
    "* **O que faz:** Retorna **apenas as linhas que têm correspondência** nas duas tabelas.\n",
    "* **Exemplo:** Quero ver só os clientes que **realmente fizeram compras**.\n",
    "* **Sintaxe PySpark:** `df_A.join(df_B, \"chave_comum\", \"inner\")`\n",
    "\n",
    "#### 2. `LEFT JOIN` (Junção Esquerda)\n",
    "\n",
    "* **O que faz:** Retorna **TUDO** da Tabela A (esquerda) e só o que combina da Tabela B (direita).\n",
    "* **Exemplo:** Quero ver **todos os clientes**, mesmo aqueles que **nunca compraram** (os campos da Tabela B ficam `null`).\n",
    "* **Sintaxe PySpark:** `df_A.join(df_B, \"chave_comum\", \"left\")`\n",
    "\n",
    "#### 3. `RIGHT JOIN` (Junção Direita)\n",
    "\n",
    "* **O que faz:** Retorna **TUDO** da Tabela B (direita) e só o que combina da Tabela A (esquerda).\n",
    "* **Exemplo:** Quero ver **todas as vendas**, mesmo que o cliente não esteja cadastrado (os campos da Tabela A ficam `null`).\n",
    "* **Sintaxe PySpark:** `df_A.join(df_B, \"chave_comum\", \"right\")`\n",
    "\n",
    "#### 4. `FULL OUTER JOIN` (Junção Externa Completa)\n",
    "\n",
    "* **O que faz:** Retorna **TUDO** o que existe em **ambas** as tabelas. É a união total.\n",
    "* **Exemplo:** Quero ver a união de todos os clientes e todas as vendas, mostrando onde eles se cruzam e onde estão separados.\n",
    "* **Sintaxe PySpark:** `df_A.join(df_B, \"chave_comum\", \"full\")`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45692a61-ae92-41df-bf32-926e864125d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_clientes.join(df_vendas, \"id_cliente\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f50b5e-660d-46d6-b6bf-959187be82a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n|id_cliente|nome |valor|\n+----------+-----+-----+\n|101       |Alice|50.0 |\n|102       |Bob  |NULL |\n+----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Tabelas de teste\n",
    "df_clientes = spark.createDataFrame([(101, \"Alice\"), (102, \"Bob\")], [\"id_cliente\", \"nome\"])\n",
    "df_vendas = spark.createDataFrame([(101, 50.0), (103, 75.0)], [\"id_cliente\", \"valor\"])\n",
    "\n",
    "# APLICAÇÃO DO LEFT JOIN\n",
    "df_final = df_clientes.join(df_vendas, \"id_cliente\", \"left\")\n",
    "\n",
    "df_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df170b5d-df04-4345-810b-1aa7fab048d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n|id_cliente|nome |valor|\n+----------+-----+-----+\n|101       |Alice|50.0 |\n+----------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Tabelas de teste\n",
    "df_clientes = spark.createDataFrame([(101, \"Alice\"), (102, \"Bob\")], [\"id_cliente\", \"nome\"])\n",
    "df_vendas = spark.createDataFrame([(101, 50.0), (103, 75.0)], [\"id_cliente\", \"valor\"])\n",
    "\n",
    "# APLICAÇÃO DO INNER JOIN\n",
    "df_final = df_clientes.join(df_vendas, \"id_cliente\", \"inner\")\n",
    "\n",
    "df_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53da6f49-d53c-4eba-83b2-401cfc4879fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Enriquecimento de Dados\n",
    "**O que é:**\n",
    "\n",
    "Adicionar novas informações para melhorar o dataset.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- Tabela de CEP → adicionar cidade e estado\n",
    "- Tabela de câmbio → converter dólar para real\n",
    "- Criar coluna de faixa etária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97591c07-b959-4d65-b9b2-d638fd32a2c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"faixa_etaria\",\n",
    "    when(col(\"idade\") < 18, \"jovem\")\n",
    "    .when(col(\"idade\") < 60, \"adulto\")\n",
    "    .otherwise(\"idoso\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f8460d-2d4c-407a-a480-cd8191ee9b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|idade|faixa_etaria|\n+-----+------------+\n|15   |jovem       |\n|25   |adulto      |\n|59   |adulto      |\n|60   |idoso       |\n|80   |idoso       |\n+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# --- Testando todas as faixas) ---\n",
    "dados_teste = [\n",
    "    (15,),   # Jovem\n",
    "    (25,),   # Adulto\n",
    "    (59,),   # Adulto (Limite)\n",
    "    (60,),   # Idoso (Começa aqui)\n",
    "    (80,)    # Idoso\n",
    "]\n",
    "df_teste = spark.createDataFrame(dados_teste, [\"idade\"])\n",
    "\n",
    "# --- APLICAÇÃO DA LÓGICA DE ENRIQUECIMENTO ---\n",
    "df_resultado = df_teste.withColumn(\n",
    "    \"faixa_etaria\",\n",
    "    when(col(\"idade\") < 18, \"jovem\")   # Se for menor que 18\n",
    "    .when(col(\"idade\") < 60, \"adulto\") # Senão, se for menor que 60\n",
    "    .otherwise(\"idoso\")                 # Senão (se for 60 ou mais)\n",
    ")\n",
    "\n",
    "df_resultado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4860e826-4cdd-4707-8aba-ebffbf6a7992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9. Pivot/Unpivot (Transposição)\n",
    "**O que é:**\n",
    "\n",
    "Transformar linhas em colunas e vice-versa.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- Vendas por mês → colunas Jan, Fev, Mar\n",
    "- Transformar métricas em linhas para facilitar leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee02b56-67ac-4472-b942-ee2d09dea180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resultado do Pivot ---\n+-------+----+---+\n|produto|Fev |Jan|\n+-------+----+---+\n|A      |50  |100|\n|B      |NULL|200|\n+-------+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 2. Dados de Entrada: (Produto, Mês, Valor)\n",
    "dados_teste = [\n",
    "    (\"A\", \"Jan\", 100),\n",
    "    (\"A\", \"Fev\", 50),\n",
    "    (\"B\", \"Jan\", 200)\n",
    "]\n",
    "df_teste = spark.createDataFrame(dados_teste, [\"produto\", \"mes\", \"valor\"])\n",
    "\n",
    "# Agrupa por 'produto', transforma 'mes' em colunas, soma 'valor'.\n",
    "df_pivot = df_teste.groupBy(\"produto\").pivot(\"mes\").sum(\"valor\")\n",
    "\n",
    "print(\"--- Resultado do Pivot ---\")\n",
    "df_pivot.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1973246d-3db9-48fc-a6fe-a56960d7407c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 10. Deduplicação\n",
    "**O que é:**\n",
    "\n",
    "Remover registros repetidos, muito comum em arquivos de origem ruim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ad180d-5fa0-4c10-8c88-c6a226a54e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropDuplicates([\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5a095a-36cd-4311-84bc-d9331002a914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DataFrame Original (Com Duplicatas) ---\n+---+------+-----+\n|id |nome  |renda|\n+---+------+-----+\n|101|Alice |5000 |\n|102|Bob   |3000 |\n|101|Alice |5500 |\n|103|Carlos|8000 |\n+---+------+-----+\n\n--- DataFrame Após dropDuplicates(['id']) ---\n+---+------+-----+\n|id |nome  |renda|\n+---+------+-----+\n|101|Alice |5000 |\n|102|Bob   |3000 |\n|103|Carlos|8000 |\n+---+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- Duplicatas no ID 101) ---\n",
    "dados_teste = [\n",
    "    (101, \"Alice\", 5000),   # Linha que será MANTIDA\n",
    "    (102, \"Bob\", 3000),     # Linha Única\n",
    "    (101, \"Alice\", 5500),   # Duplicata que será REMOVIDA\n",
    "    (103, \"Carlos\", 8000)   # Linha Única\n",
    "]\n",
    "df_teste = spark.createDataFrame(dados_teste, [\"id\", \"nome\", \"renda\"])\n",
    "\n",
    "print(\"--- DataFrame Original (Com Duplicatas) ---\")\n",
    "df_teste.show(truncate=False)\n",
    "\n",
    "# Remove duplicatas baseadas APENAS na coluna \"id\"\n",
    "df_resultado = df_teste.dropDuplicates([\"id\"])\n",
    "\n",
    "\n",
    "print(\"--- DataFrame Após dropDuplicates(['id']) ---\")\n",
    "df_resultado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bd5f76-239d-4ae3-a75e-db7413d65e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 11. Regras de Negócio\n",
    "**O que é:**\n",
    "\n",
    "Criar colunas derivadas com lógicas definidas pelo time de negócios.\n",
    "\n",
    "Exemplos:\n",
    "\n",
    "- Valor > 10.000 → marcar como transação suspeita\n",
    "- Cliente sem e-mail → marcar como “incompleto”\n",
    "- Entrega atrasada → atraso = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2324414-3895-41b8-a98b-dad9e20df894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"suspeita\", col(\"valor\") > 10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4279772e-2d2d-48be-9ce0-331da7cd55f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resultado da Regra de Suspeita ---\n+--------+--------+\n|valor   |suspeita|\n+--------+--------+\n|5000.0  |false   |\n|10000.01|true    |\n|25000.0 |true    |\n+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dados_teste = [\n",
    "    (5000.00,),   # Não suspeito\n",
    "    (10000.01,),  # Suspeito (maior que 10000)\n",
    "    (25000.00,)   # Suspeito\n",
    "]\n",
    "df_teste = spark.createDataFrame(dados_teste, [\"valor\"])\n",
    "\n",
    "# REGRA DE NEGÓCIO\n",
    "# Cria a coluna 'suspeita'\n",
    "df_resultado = df_teste.withColumn(\"suspeita\", col(\"valor\") > 10000)\n",
    "\n",
    "print(\"--- Resultado da Regra de Suspeita ---\")\n",
    "df_resultado.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7499368503499663,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Manipulacao_e_IO_Basico",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}