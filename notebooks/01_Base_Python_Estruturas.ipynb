{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1bb060a-267f-432a-b186-76a4cff68004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **1. Estruturas Fundamentais da Linguagem (Python Básico)**\n",
    "*(Notebook : `01_Base_Python_Estruturas`)*\n",
    "\n",
    "1 - Variáveis** (Guardar valores)\n",
    "\n",
    "2 - Listas** (Vários valores)\n",
    "\n",
    "3 - Dicionário** (Estrutura `Chave: Valor`, similar a JSON)\n",
    "\n",
    "4 - Condição** (`if` para tomada de decisão)\n",
    "\n",
    "5 - Loop** (`for` para repetição de ações)\n",
    "\n",
    "6 - Função** (`def` para criar código reutilizável)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad6d57ca-be3d-495b-895d-a3f6bdd8a790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1. Variáveis (guardar valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07544616-e49a-4e4e-8b79-b58cd22682a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maria 30 50\n"
     ]
    }
   ],
   "source": [
    "nome = \"Maria\"\n",
    "idade = 30\n",
    "peso = 50\n",
    "\n",
    "print(nome, idade, peso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd28c58d-febc-48d2-a5c9-051fe18710c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. Listas (vários valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b353459d-f3d9-4a98-a9d8-f7c4bcb71dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maçã', 'banana', 'uva']\n"
     ]
    }
   ],
   "source": [
    "frutas = [\"maçã\", \"banana\", \"uva\"]\n",
    "print(frutas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33d2b261-465f-45c3-837e-e4a3bafa89eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3. Dicionário NO PySpark\n",
    "\n",
    "-  Representar dados estruturados de forma clara usando chave → valor.\n",
    "-  Facilitar leitura de JSON e APIs.\n",
    "-  Criar DataFrames onde o nome das colunas já vem do próprio dicionário.\n",
    "-  Evitar erros de ordem dos campos.\n",
    "-  Ser mais natural para dados complexos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854005c8-644f-4218-acac-3c99af147b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####FORMA 1 — Lista de listas + Schema (MELHOR PRÁTICA!)\n",
    "\n",
    "**Quando usar?**\n",
    "- Em produção\n",
    "- Quando você precisa garantir os tipos\n",
    "- Quando o arquivo real vai seguir esse schema\n",
    "\n",
    "**Como funciona?**\n",
    "- Cada lista interna é uma linha\n",
    "- O schema define o tipo de cada coluna\n",
    "- O Spark NÃO precisa adivinhar nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0638764-ea67-417d-99f9-6a9777792e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n|   nome|sobrenome|idade|\n+-------+---------+-----+\n|  Maria|    Silva|   25|\n|   João| Oliveira|   35|\n| Tereza|   Santos|   29|\n|Antonio|     Lima|   31|\n+-------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemplo Forma 1\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# DEFININDO O SCHEMA MANUALMENTE (melhor prática)\n",
    "schema = StructType([\n",
    "    StructField(\"nome\", StringType()),        # Coluna 1\n",
    "    StructField(\"sobrenome\", StringType()),   # Coluna 2\n",
    "    StructField(\"idade\", IntegerType())       # Coluna 3\n",
    "])\n",
    "\n",
    "# CADA LISTA = UMA LINHA\n",
    "dados = [\n",
    "    [\"Maria\", \"Silva\", 25],\n",
    "    [\"João\", \"Oliveira\", 35],\n",
    "    [\"Tereza\", \"Santos\", 29],\n",
    "    [\"Antonio\", \"Lima\", 31]\n",
    "]\n",
    "\n",
    "# CRIANDO O DATAFRAME COM SCHEMA DEFINIDO\n",
    "df = spark.createDataFrame(dados, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49527bf-1712-4833-bbd2-b2e3bfddb660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####FORMA 2 — Lista de tuplas + nomes das colunas\n",
    "\n",
    "**Quando usar?**\n",
    "- Protótipos\n",
    "- Estudos\n",
    "- Quando não precisa de schema detalhado\n",
    "- Criação muito rápida\n",
    "\n",
    "**Como funciona?**\n",
    "- Tupla = linha\n",
    "- Você só informa o nome das colunas\n",
    "- O Spark infere automaticamente o tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865b5a11-fdfa-4eae-8037-f1c7b1b6582b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n|   nome|sobrenome|idade|\n+-------+---------+-----+\n|  Maria|    Silva|   25|\n|   João| Oliveira|   35|\n| Tereza|   Santos|   29|\n|Antonio|     Lima|   31|\n+-------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exemplo Forma 2       \n",
    "\n",
    "# LISTA DE TUPLAS\n",
    "# Tupla = linha + imutável (não deixa você alterar sem querer)\n",
    "dados = [\n",
    "    (\"Maria\", \"Silva\", 25),\n",
    "    (\"João\", \"Oliveira\", 35),\n",
    "    (\"Tereza\", \"Santos\", 29),\n",
    "    (\"Antonio\", \"Lima\", 31)\n",
    "]\n",
    "\n",
    "# NOMES DAS COLUNAS\n",
    "colunas = [\"nome\", \"sobrenome\", \"idade\"]\n",
    "\n",
    "# SPARK INFERE O TIPO AUTOMATICAMENTE\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2662330-e43f-4cdb-8f2d-f2ddd28c2a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####FORMA 3 — Lista de dicionários (parecido com JSON)\n",
    "**Quando usar?**\n",
    "- Quando os dados vêm de API (JSON)\n",
    "- Quando já estão em dicts\n",
    "- Quando você quer deixar o código legível\n",
    "\n",
    "**Como funciona?**\n",
    "- Cada dicionário é uma linha\n",
    "- As chaves viram colunas\n",
    "- O Spark infere o tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a660b6b-f248-4d33-bd20-4b54f420c6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+\n|idade|   nome|sobrenome|\n+-----+-------+---------+\n|   25|  Maria|    Silva|\n|   35|   João| Oliveira|\n|   29| Tereza|   Santos|\n|   31|Antonio|     Lima|\n+-----+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Exemplo Forma 3\n",
    "\n",
    "# CADA DICIONÁRIO REPRESENTA UMA LINHA\n",
    "# A CHAVE SEMPRE VIRA NOME DA COLUNA\n",
    "dados = [\n",
    "    {\n",
    "        \"nome\": \"Maria\",\n",
    "        \"sobrenome\": \"Silva\",\n",
    "        \"idade\": 25\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"João\",\n",
    "        \"sobrenome\": \"Oliveira\",\n",
    "        \"idade\": 35\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"Tereza\",\n",
    "        \"sobrenome\": \"Santos\",\n",
    "        \"idade\": 29\n",
    "    },\n",
    "    {\n",
    "        \"nome\": \"Antonio\",\n",
    "        \"sobrenome\": \"Lima\",\n",
    "        \"idade\": 31\n",
    "    }\n",
    "]\n",
    "\n",
    "# SPARK ENTENDE AS CHAVES COMO COLUNAS E OS VALORES COMO LINHAS\n",
    "df = spark.createDataFrame(dados)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c403d07d-867d-42b1-8d2c-d5ada0ad7805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## QUAL FORMA USAR? (resumo rápido)\n",
    "\n",
    "| Forma                | Melhor para             | Controle de tipos | Facilidade |\n",
    "|----------------------|--------------------------|-------------------|------------|\n",
    "| 1 — Lista + Schema   | Produção, ingestão séria | ⭐⭐⭐⭐⭐            | ⭐⭐         |\n",
    "| 2 — Tuplas           | Testes rápidos           | ⭐⭐                | ⭐⭐⭐⭐⭐     |\n",
    "| 3 — Dicionários      | Dados vindos de JSON     | ⭐⭐⭐               | ⭐⭐⭐⭐      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c101cc88-e48c-486c-a64a-dccd092d1d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###4. Condição (if)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a14b7a-860c-43d3-9280-42c5473fed94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O que é uma condição (if) em Python?\n",
    "O if é usado quando você quer que o código tome decisões.\n",
    "\n",
    "Ele funciona como:\n",
    "- Se uma condição for verdadeira → faz algo\n",
    "- Senão → faz outra coisa\n",
    "\n",
    "É a mesma lógica que você usa na vida real:\n",
    "- Se chover, levo guarda-chuva.\n",
    "- Senão, saio sem.\n",
    "\n",
    "Usar lógica condicional o tempo todo, por exemplo:\n",
    "- Validar dados\n",
    "- Tratar valores nulos\n",
    "- Criar colunas derivadas\n",
    "- Fazer regras para qualidade de dados\n",
    "- Testar pipelines\n",
    "\n",
    "**Tópicos**\n",
    "1. elif (mais de uma condição)\n",
    "2. operadores lógicos (and / or)\n",
    "3. if dentro de função\n",
    "4. como usar if no PySpark (when, otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81407c42-7387-4df2-b363-aa5943f77a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menor de idade\n"
     ]
    }
   ],
   "source": [
    "idade = 17\n",
    "\n",
    "if idade >= 18:\n",
    "    print(\"Maior de idade\")\n",
    "else:\n",
    "    print(\"Menor de idade\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51cc1200-b3e2-4f8a-88f0-35d06aed06f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### if aplicada dentro de um DataFrame.\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"classificacao\",\n",
    "    when(col(\"idade\") >= 18, \"adulto\").otherwise(\"menor\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6011af19-9950-4aed-828e-a5f775d4f909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adolescente\n"
     ]
    }
   ],
   "source": [
    "## 1. elif — Quando você tem mais de uma condição\n",
    "\n",
    "## O elif significa “senão, se…”.\n",
    "## Ele é usado quando você precisa testar mais de duas possibilidades.\n",
    "\n",
    "#Exemplo \n",
    "idade = 17\n",
    "\n",
    "if idade < 12:\n",
    "    print(\"Criança\")\n",
    "elif idade < 18:\n",
    "    print(\"Adolescente\")\n",
    "elif idade < 60:\n",
    "    print(\"Adulto\")\n",
    "else:\n",
    "    print(\"Idoso\")\n",
    "\n",
    "## Obs:\n",
    "## O Python testa de cima pra baixo.\n",
    "## Assim que encontra uma condição verdadeira, ele para e executa só aquele bloco.\n",
    "## O elif evita que você escreva vários if separados (o que seria errado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263383cc-ca71-4def-bee6-c005c820f9d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pode dirigir\nTem direito a desconto\n"
     ]
    }
   ],
   "source": [
    "## 2. Operadores lógicos (and / or)\n",
    "## and = as duas condições precisam ser verdadeiras\n",
    "\n",
    "## Exemplo: pessoa deve ser maior de 18 E ter CNH\n",
    "idade = 20\n",
    "tem_cnh = True\n",
    "\n",
    "if idade >= 18 and tem_cnh:\n",
    "    print(\"Pode dirigir\")\n",
    "else:\n",
    "    print(\"Não pode dirigir\")\n",
    "\n",
    "## or = basta UMA condição ser verdadeira\n",
    "## Exemplo: pode ter desconto se for estudante OU idoso\n",
    "\n",
    "estudante = False\n",
    "idoso = True\n",
    "\n",
    "if estudante or idoso:\n",
    "    print(\"Tem direito a desconto\")\n",
    "else:\n",
    "    print(\"Não tem desconto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8397e9d4-cb37-4c43-9331-c7f4a51921e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "## 3. If dentro de função\n",
    "## Você usa if dentro de funções quando quer que elas tomem decisões.\n",
    "\n",
    "## Exemplo \n",
    "def calcular_preco(valor, eh_vip):\n",
    "    if eh_vip:\n",
    "        return valor * 0.8  # 20% de desconto\n",
    "    else:\n",
    "        return valor\n",
    "\n",
    "preco = calcular_preco(100, True)\n",
    "print(preco)  # 80\n",
    "\n",
    "## Obs:\n",
    "## - A função recebe valores.\n",
    "## - O if decide o que fazer com eles.\n",
    "## - E retorna o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e3b3e5-59a0-4534-a1d1-a502de2091a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n|idade|faixa_etaria|\n+-----+------------+\n|   10|     Criança|\n|   15| Adolescente|\n|   30|      Adulto|\n|   80|       Idoso|\n+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "## 4. If no PySpark (when, otherwise)\n",
    "## Em PySpark você não usa if direto, porque Spark trabalha com colunas, não com valores individuais.\n",
    "\n",
    "## when → equivalente ao if\n",
    "## otherwise → equivalente ao else\n",
    "\n",
    "## Exemplo  (classificar idade)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# criar/get SparkSession (em Databricks já existe 'spark', então essa linha pode ser desnecessária)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# criar DataFrame de exemplo com uma coluna 'idade'\n",
    "df = spark.createDataFrame(\n",
    "    [(10,), (15,), (30,), (80,)],\n",
    "    [\"idade\"]\n",
    ")\n",
    "# agora aplica a lógica de faixa etária\n",
    "df = df.withColumn(\n",
    "    \"faixa_etaria\",\n",
    "    F.when(F.col(\"idade\") < 12, \"Criança\")\n",
    "     .when(F.col(\"idade\") < 18, \"Adolescente\")\n",
    "     .when(F.col(\"idade\") < 60, \"Adulto\")\n",
    "     .otherwise(\"Idoso\")\n",
    ")\n",
    "df.show()\n",
    "\n",
    "## Obs:\n",
    "## É exatamente igual ao:\n",
    "## - if\n",
    "## - elif\n",
    "## - elif\n",
    "## - else\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db19622-a00b-4086-83cc-fac6099c95ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5. Loop (for)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6acfc98-24da-4ecb-a83b-de6905ee4387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "O loop for serve para repetir uma ação várias vezes.\n",
    "Ele “passa por cada item” de uma lista, tupla, string, etc.\n",
    "\n",
    "Você usa para:\n",
    "- percorrer uma lista\n",
    "- executar algo várias vezes\n",
    "- gerar valores repetidos\n",
    "- criar dados para teste\n",
    "- montar listas para DataFrames\n",
    "\n",
    "Como o “for” aparece no PySpark na prática:\n",
    "1. Quando criamos uma lista para um DataFrame:\n",
    "2. Quando geramos colunas repetitivas para teste:\n",
    "3. Quando lemos vários arquivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bea1769-79ec-4847-8901-484fc9a94311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  valor\n0   0      0\n1   1     10\n2   2     20\n3   3     30\n4   4     40\n"
     ]
    }
   ],
   "source": [
    "## 1. Quando criamos uma lista para um DataFrame:\n",
    "## Usamos for para gerar dados automaticamente, em vez de digitar um por um.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Criar lista de dados automaticamente\n",
    "dados = []\n",
    "for i in range(5):\n",
    "    dados.append((i, i*10))\n",
    "\n",
    "# 2. Transformar a lista em DataFrame\n",
    "df = pd.DataFrame(dados, columns=[\"id\", \"valor\"])\n",
    "print(df)\n",
    "\n",
    "## Obs: Serve para criar dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1921dda8-059a-4cb9-9cca-fca6a69a5617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+\n|col_0|col_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+-----+\n|    0|    1|    2|    3|    4|\n+-----+-----+-----+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "## 2. Quando geramos colunas repetitivas para teste:\n",
    "\n",
    "\"\"\"\n",
    "Explicação: Em vez de escrever\n",
    "    \n",
    "df = df.select(\n",
    "    F.lit(0).alias(\"col_0\"),\n",
    "    F.lit(1).alias(\"col_1\"),\n",
    "    F.lit(2).alias(\"col_2\"),\n",
    "    ...\n",
    "    F.lit(50).alias(\"col_50\")\n",
    ")\n",
    "Você usa um for que cria tudo automaticamente.\n",
    "\n",
    "Está fazendo exatamente isso:\n",
    "- Criando 5 colunas automaticamente\n",
    "- Cada coluna tem um valor fixo (lit(i))\n",
    "- Cada coluna tem nome automático (col_0, col_1, ...)\n",
    "\n",
    "Depois o select monta o DataFrame com essas colunas(como no exemplo abaixo)\n",
    "\n",
    "colunas = []\n",
    "for i in range(5):\n",
    "    colunas.append(F.lit(i).alias(f\"col_{i}\"))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Criando lista de colunas automáticas\n",
    "colunas = []\n",
    "for i in range(5):\n",
    "    colunas.append(F.lit(i).alias(f\"col_{i}\"))\n",
    "\n",
    "# 2. Criando um DF simples com 1 linha\n",
    "df = spark.createDataFrame([(1,)])\n",
    "\n",
    "# 3. Substituindo pela lista de colunas\n",
    "df = df.select(*colunas)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0203b8a-ed5a-4274-ae0a-c43c117d6790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 3. Quando lemos vários arquivos:\n",
    "## Você tem vários arquivos para ler (ex: jan.csv, fev.csv, mar.csv).\n",
    "arquivos = [\"jan.csv\", \"fev.csv\", \"mar.csv\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "- Sem for:\n",
    "Você teria que fazer:\n",
    "    spark.read.csv(\"jan.csv\")\n",
    "    spark.read.csv(\"fev.csv\")\n",
    "    spark.read.csv(\"mar.csv\")\n",
    "\n",
    "- Com for:\n",
    "Você deixa o Python fazer isso sozinho:\n",
    "arquivos = [\"jan.csv\", \"fev.csv\", \"mar.csv\"]\n",
    "\n",
    "for arquivo in arquivos:\n",
    "    df = spark.read.csv(arquivo)\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Lista com os nomes dos arquivos que queremos ler\n",
    "arquivos = [\"dados_1.csv\", \"dados_2.csv\", \"dados_3.csv\"]\n",
    "\n",
    "# Lista vazia onde vamos guardar cada DataFrame lido\n",
    "dfs = []\n",
    "\n",
    "# Loop para ler cada arquivo da lista\n",
    "for arquivo in arquivos:\n",
    "    \n",
    "    # Lê um arquivo CSV e cria um DataFrame temporário\n",
    "    df_temp = spark.read.csv(arquivo, header=True)\n",
    "    \n",
    "    # Coloca o DataFrame temporário dentro da lista\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# Começamos o DataFrame final com o primeiro DataFrame da lista\n",
    "df_final = dfs[0]\n",
    "\n",
    "# Fazemos UNION com os demais DataFrames (juntando tudo)\n",
    "for df in dfs[1:]:\n",
    "    df_final = df_final.unionByName(df)\n",
    "\n",
    "# Mostra o DataFrame final já com todos os arquivos juntos\n",
    "# df_final.show()\n",
    "\n",
    "\n",
    "##O for lê todos os arquivos automaticamente, e depois fazemos um “union” para juntar tudo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4b034d-4ca2-4cec-8453-3d4e985f767e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6. Função"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66077dab-7f7c-47a1-9fd4-0f24d593f9e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Uma função é um bloco de código que você cria para reutilizar em vários lugares.\n",
    "\n",
    "Em vez de repetir o mesmo código 10 vezes, você cria UMA função e chama quando precisar.Uma função é um bloco de código que você cria para reutilizar em vários lugares.\n",
    "\n",
    "Em vez de repetir o mesmo código 10 vezes, você cria UMA função e chama quando precisar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1801b94-257f-4b4b-9aec-6c645e795da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo 1 — Função simples\n",
    "\n",
    "# Criamos uma função chamada processar_df\n",
    "def processar_df(df):\n",
    "    # Remove linhas duplicadas\n",
    "    df = df.dropDuplicates()\n",
    "    # Remove linhas com valores nulos\n",
    "    df = df.na.drop()\n",
    "    # Retorna o DataFrame limpo\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1883d2dc-221b-43f3-8a3b-2c16d77a830d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Exemplo 2. Como usar a função \n",
    "\n",
    "# Aqui estamos chamando a função processar_df\n",
    "# Passando como argumento o DataFrame df_original\n",
    "\n",
    "#df_limpo = processar_df(df_original)\n",
    "\n",
    "# Agora df_limpo recebe o resultado da função (um DataFrame limpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13108322-3c97-4f4e-9dbd-243b0a9f1807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo 3: função mais completa\n",
    "\n",
    "# Criamos uma função chamada limpar_e_renomear\n",
    "def limpar_e_renomear(df):\n",
    "    # Remove linhas duplicadas\n",
    "    df = df.dropDuplicates()\n",
    "    # Remove linhas com valores nulos\n",
    "    df = df.na.drop()\n",
    "    # Renomeia a coluna \"col1\" para \"id\"\n",
    "    df = df.withColumnRenamed(\"col1\", \"id\")\n",
    "    # Retorna o DataFrame modificado\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d073fd1-ed88-49a3-8bcc-1e59caf0db81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Exercícios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e008ad-87b2-426c-9ee7-acb99ec0ffbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXERCÍCIO 1 — Função simples em Python\n",
    "Crie uma função chamada dobrar que recebe um número e retorna ele vezes 2.\n",
    "use: \n",
    "\n",
    "def nome_da_funcao(parametro):\n",
    "    return alguma_coisa\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Resultado:\n",
    "# Função que dobra um número\n",
    "# Recebe um número e devolve ele multiplicado por 2.\n",
    "\n",
    "# Codigo limpo\n",
    "# def dobrar(numero):\n",
    "#     return numero * 2\n",
    "\n",
    "# Perfumaria nao roda \n",
    "# 'def' cria a função, 'dobrar' é o nome, e 'numero' é o valor que ela recebe.\n",
    "def dobrar(numero):\n",
    "    # A função calcula o valor recebido (numero) vezes 2, e devolve o resultado.\n",
    "    return numero * 2\n",
    "\n",
    "# CHAMANDO A FUNÇÃO\n",
    "# A função 'dobrar' é chamada com o valor 5.\n",
    "# O resultado (10) é guardado na variável 'resultado'.\n",
    "resultado = dobrar(5)\n",
    "# Exibe o valor que está em 'resultado'.\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71316a16-caea-41bb-afcc-2558b9b8b436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A área do quadrado é: 16\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXERCÍCIO 2 — Função para somar 2 números\n",
    "Crie uma função:\n",
    "somar(a, b)\n",
    "que retorna a + b.\n",
    "Dica:\n",
    "A função deve ter 2 parâmetros.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Resultado\n",
    "# Função que soma dois números\n",
    "# Recebe dois valores e devolve a soma.\n",
    "\n",
    "# Codigo limpo\n",
    "# def somar(a, b):\n",
    "#     return a + b\n",
    "\n",
    "\n",
    "\n",
    "# Perfumaria nao roda \n",
    "# 'area_quadrado' é o nome. Ela recebe apenas um valor: o 'lado'.\n",
    "def area_quadrado(lado):\n",
    "    # O cálculo é lado * lado. O resultado é devolvido.\n",
    "    return lado * lado\n",
    "\n",
    "# A função é chamada com 4 (que é o lado).\n",
    "# O resultado (4 * 4 = 16) é armazenado.\n",
    "resultado = area_quadrado(4)\n",
    "\n",
    "# Mostra o valor 16 (a área do quadrado de lado 4).\n",
    "print(f\"A área do quadrado é: {resultado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9fd2b08-bcc6-4253-8a54-ea9961310cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXERCÍCIO 3 — Função que limpa um DataFrame no PySpark\n",
    "Crie uma função chamada: limpar(df)\n",
    "Ela deve:\n",
    "- remover duplicados\n",
    "- remover nulos\n",
    "- retornar o DataFrame limpo\n",
    "Dica:\n",
    "Use:\n",
    "df.dropDuplicates()\n",
    "df.na.drop()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Resultado\n",
    "# Função que limpa um DataFrame no PySpark\n",
    "# Essa função é muito comum em ETL.\n",
    "\n",
    "# Codigo limpo \n",
    "def limpar(df):\n",
    "    df = df.dropDuplicates()   # Remove linhas duplicadas\n",
    "    df = df.na.drop()          # Remove linhas com valores nulos\n",
    "    return df                  # Retorna o DataFrame limpo\n",
    "\n",
    "# Perfumaria nao roda\n",
    "# Dados de entrada: uma lista de registros (id, nome).\n",
    "# dados = [\n",
    "#     (1, \"Maria\"),\n",
    "#     (1, \"Maria\"),  # Exemplo de dado REPETIDO\n",
    "#     (2, None)      # Exemplo de dado VAZIO (nulo)\n",
    "# ]\n",
    "# # Nomes dados às colunas da tabela.\n",
    "# colunas = [\"id\", \"nome\"]\n",
    "\n",
    "# # CRIAÇÃO DA TABELA (DATAFRAME)\n",
    "# # Pega os 'dados' e 'colunas' e cria a tabela 'df' no PySpark.\n",
    "# df = spark.createDataFrame(dados, colunas)\n",
    "# # Mostra a tabela 'df' ORIGINAL (com lixo).\n",
    "# df.show()\n",
    "\n",
    "# # LIMPEZA\n",
    "# # 'limpar' é uma função que remove o lixo (duplicatas e nulos) de 'df'.\n",
    "# # O resultado da limpeza é salvo em uma NOVA tabela chamada 'df_limpo'.\n",
    "# df_limpo = limpar(df)\n",
    "# # Mostra a tabela 'df_limpo' DEPOIS de ter sido limpa.\n",
    "# df_limpo.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8153c6-b920-4328-b0ea-3372463e681e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resultado Final da Classificação ---\n+---+-----+------------+\n| id|idade|faixa_etaria|\n+---+-----+------------+\n|  1|   10|     Criança|\n|  2|   17| Adolescente|\n|  3|   35|      Adulto|\n|  4|   65|       Idoso|\n+---+-----+------------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXERCÍCIO 4 — Função que cria coluna com faixa etária\n",
    "Crie uma função chamada: adicionar_faixa(df)\n",
    "\n",
    "Que adiciona uma coluna chamada faixa_etaria com:\n",
    "- idade < 12 → \"Criança\"\n",
    "- idade < 18 → \"Adolescente\"\n",
    "- idade < 60 → \"Adulto\"\n",
    "- senão → \"Idoso\"\n",
    "\n",
    "\uD83D\uDD0E Dica:\n",
    "Use:\n",
    "F.when().otherwise()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Resultado\n",
    "# Função que adiciona coluna de faixa etária\n",
    "# É o equivalente ao if/elif/else no PySpark.\n",
    "\n",
    "\n",
    "# Codigo limpo\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# def adicionar_faixa(df):\n",
    "#     return df.withColumn(\n",
    "#         \"faixa_etaria\",\n",
    "#         F.when(F.col(\"idade\") < 12, \"Criança\")\n",
    "#          .when(F.col(\"idade\") < 18, \"Adolescente\")\n",
    "#          .when(F.col(\"idade\") < 60, \"Adulto\")\n",
    "#          .otherwise(\"Idoso\")\n",
    "#     )\n",
    "\n",
    "# Perfumaria \n",
    "# Importa as funções do PySpark e dá a elas o apelido 'F'.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# CRIAÇÃO DOS DADOS DE TESTE (df_entrada) \n",
    "# Dados de exemplo: (id, idade)\n",
    "dados_teste = [\n",
    "    (1, 10),  # Criança\n",
    "    (2, 17),  # Adolescente\n",
    "    (3, 35),  # Adulto\n",
    "    (4, 65)   # Idoso\n",
    "]\n",
    "colunas = [\"id\", \"idade\"]\n",
    "\n",
    "# Cria o DataFrame (Tabela) de entrada que será passado para a função\n",
    "df_entrada = spark.createDataFrame(dados_teste, colunas)\n",
    "\n",
    "# DEFINIÇÃO DA SUA FUNÇÃO ---\n",
    "def adicionar_faixa(df):\n",
    "    return df.withColumn(\n",
    "        \"faixa_etaria\",\n",
    "        F.when(F.col(\"idade\") < 12, \"Criança\")\n",
    "        .when(F.col(\"idade\") < 18, \"Adolescente\")\n",
    "        .when(F.col(\"idade\") < 60, \"Adulto\")\n",
    "        .otherwise(\"Idoso\")\n",
    "    )\n",
    "\n",
    "# EXECUÇÃO E IMPRESSÃO ---\n",
    "# Chama a função, passando o DataFrame de entrada e salva o resultado\n",
    "df_resultado = adicionar_faixa(df_entrada)\n",
    "\n",
    "# Imprime o DataFrame resultante na tela\n",
    "print(\"--- Resultado Final da Classificação ---\")\n",
    "df_resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f6ffda-fbf9-45d6-b825-2a30e71c5305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXERCÍCIO 5 — Função que lê vários arquivos\n",
    "Crie uma função: ler_varios_arquivos(caminhos)\n",
    "\n",
    "Que recebe uma lista de caminhos e retorna um único DataFrame com:\n",
    "df = spark.read.csv(caminho)\n",
    "\n",
    "e junta tudo com:\n",
    "df_final = df_final.union(df)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Resultado\n",
    "# Função que lê vários arquivos e junta tudo\n",
    "# Lê vários arquivos e faz um “union” de todos.\n",
    "\n",
    "\n",
    "#codigo limpo\n",
    "def ler_varios_arquivos(caminhos):\n",
    "    df_final = None   # Começa vazio\n",
    "\n",
    "    for caminho in caminhos:\n",
    "        df_temp = spark.read.csv(caminho, header=True)\n",
    "\n",
    "        if df_final is None:\n",
    "            df_final = df_temp            # Primeiro arquivo\n",
    "        else:\n",
    "            df_final = df_final.union(df_temp)   # Junta com os outros\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "\n",
    "#perfumaria nao roda \n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # 1. DADOS DE TESTE (Criados diretamente no ambiente Spark do Databricks)\n",
    "# # Criamos duas tabelas (DataFrames) que sua função irá unir.\n",
    "# # NOTE: Usamos o objeto 'spark' que já está ativo no seu cluster.\n",
    "# df_arq1 = spark.createDataFrame([(1, \"A\")], [\"id\", \"dado\"])\n",
    "# df_arq2 = spark.createDataFrame([(2, \"B\")], [\"id\", \"dado\"])\n",
    "\n",
    "# # Lista das tabelas que a função irá percorrer e unir.\n",
    "# tabelas_para_unir = [df_arq1, df_arq2]\n",
    "\n",
    "# # 2. SUA FUNÇÃO (LÓGICA DE UNION)\n",
    "# def ler_varios_arquivos(tabelas):\n",
    "#     df_final = None   \n",
    "    \n",
    "#     # Percorre cada tabela na lista\n",
    "#     for df_temp in tabelas:\n",
    "        \n",
    "#         # Se for a primeira tabela, inicia o DataFrame final\n",
    "#         if df_final is None:\n",
    "#             df_final = df_temp   \n",
    "        \n",
    "#         # Senão, empilha as linhas (UNION)\n",
    "#         else:\n",
    "#             df_final = df_final.union(df_temp)\n",
    "\n",
    "#     return df_final\n",
    "\n",
    "# # 3. EXECUÇÃO E IMPRESSÃO\n",
    "# df_resultado = ler_varios_arquivos(tabelas_para_unir)\n",
    "\n",
    "# print(\"--- Resultado Final da União ---\")\n",
    "# df_resultado.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Base_Python_Estruturas",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}